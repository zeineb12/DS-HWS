{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFB Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will begine to work with data to build some basic models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS\n",
    "\n",
    "Here are a bunch of libraries that you may need to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import math  \n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# ignore some warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed for replication\n",
    "SEED = 1  # Use this anywhere a stochastic function allows you to set a seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: LOGIT MODEL (without sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we collect data for a group of students in a course with the following variables:  \n",
    "\n",
    "    X1 = hours studied  \n",
    "    X2 = undergrad GPA  \n",
    "    Y = receive a grade of 4.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we fit a logit model to the data above and obtain the following coefficients:\n",
    "b0 = -5\n",
    "b1 = 0.1\n",
    "b2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Estimate the probability that a student who studies for 40 hours, and who has an undergrad GPA of 3.0, will get a 4.0 in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the student gets 4.0 is :  0.88\n"
     ]
    }
   ],
   "source": [
    "# Estimate the probability and print it out\n",
    "# y = 1 / (1 + exp(-βTX))\n",
    "X1 = 40\n",
    "X2 = 3.0\n",
    "prob = 1/(1 + np.exp(-(b0+ b1*X1 + b2*X2)))\n",
    "print(\"Probability that the student gets 4.0 is : \", (\"%.2f\" % prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: How many hours would a student who has an undergrad GPA of 3.0 need to study to have an 80% chance of getting a grade of 4.0 in the class? Tip: You can do all of this on paper with math. But then type it into the markdown field below to show your work. Perform the very final (reduced) calculation in python to show the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that using logistic regression computes probabilities as: $$y = \\frac{1}{1 + exp(-β^TX)}$$\n",
    "\n",
    "For the given problem X2 and y are known (X2=3.0 and we want y=0.8)\n",
    "Hence solving this equality for X1 (which is the number of hours worked) gives us:\n",
    "\n",
    "$$X1 = -\\frac{\\ln (\\frac{1}{y}-1)+b2\\times X2+b0}{b1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hours the student has to work is:  34\n"
     ]
    }
   ],
   "source": [
    "# Use python to manually calculate that value:\n",
    "X2 = 3.0\n",
    "y = 0.8\n",
    "X1 = (-1/b1)*(np.log((1/y)-1)+b2*X2+b0)\n",
    "print(\"The number of hours the student has to work is: \", int(np.round(X1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Use your solution from above (along with the estimated logit model coefficients given to you earlier) to show that the logit model would in fact predict a probability of 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the calculation (written out, step-by-step) here:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the student gets 4.0 is :  0.80\n"
     ]
    }
   ],
   "source": [
    "# Use Python to perform the calculation and show the result:\n",
    "X1 = 34\n",
    "X2 = 3.0\n",
    "prob = 1/(1 + np.exp(-(b0+ b1*X1 + b2*X2)))\n",
    "\n",
    "print(\"Probability that the student gets 4.0 is : \", (\"%.2f\" % prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: PLOT ROC CURVES (without sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have the following labeled data: for each observation you are given a Positive or Negative label and the predicted probability. The \"Positive\" case is labeled as 1, and the \"Negative\" case is labeled with 0. For example, an observation of (0, 0.45) means the truth value is Negative and the predicted probability of being positive is 0.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 0.9),\n",
    "    (0, 0.8),\n",
    "    (1, 0.6),\n",
    "    (1, 0.4),\n",
    "    (0, 0.2)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Calculate the True Positive Rate and the False Positive Rate at each observation. You may NOT use `sklearn` or any other supporting library for this problem - you must do everything in \"pure Python\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that : $$ TPR = \\frac{TP}{TP+FN} $$\n",
    "And: $$ FPR = \\frac{FP}{FP+TN} $$\n",
    "\n",
    "**We suppose that the selected threshold probability is equal to 0.5 in the following computations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPR(threshold):\n",
    "    predictions = [(x[0],1) if x[1]>=threshold else (x[0],0) for x in data]\n",
    "    TP = np.sum([1 if x==y and x==1 else 0 for (x,y) in predictions])\n",
    "    FN = np.sum([1 if x!=y and y==0 else 0 for (x,y) in predictions])\n",
    "    TPR = 0 if TP==0 else TP/(TP+FN)\n",
    "    return TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPR(threshold):\n",
    "    predictions = [(x[0],1) if x[1]>=threshold else (x[0],0) for x in data]\n",
    "    FP = np.sum([1 if x!=y and y==1 else 0 for (x,y) in predictions])\n",
    "    TN = np.sum([1 if x==y and x==0 else 0 for (x,y) in predictions])\n",
    "    FPR = 0 if FP ==0 else FP/(FP+TN)\n",
    "    return FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR =  0.67 FPR =  0.50\n"
     ]
    }
   ],
   "source": [
    "# Calculate TPR and FPR\n",
    "threshold = 0.5\n",
    "print(\"TPR = \", (\"%.2f\" % TPR(threshold)), \"FPR = \", (\"%.2f\" % FPR(threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Plot an ROC curve. You may NOT use `sklearn` or any other supporting library for this problem - you must do everything in \"pure Python\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ROC curves is a plot of the TPR against the FPR measured for different thresolds at each point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the ROC Curve\n",
    "thresholds = np.arange(0,1,0.01)\n",
    "x = [FPR(threshold) for threshold in thresholds]\n",
    "y = [TPR(threshold) for threshold in thresholds]\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC curve\")\n",
    "plt.plot(x,y)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: AGGLOMERATIVE CLUSTERING (without sklearn)\n",
    "\n",
    "Suppose you have the following observations in a two-dimensional Euclidean space. Here we give you a dictionary with a label for the point, and the point in (x,y) coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw observations\n",
    "data   = [(1,1), (2,1), (5,1), (7,1), (1,5), (3,4), (4,5) ]\n",
    "labels = [ 'A',   'B',   'C',   'D',   'E',   'F',   'G']\n",
    "\n",
    "# Clusters of 1 observation, at lowest level\n",
    "a = [data[0], ]\n",
    "b = [data[1], ]\n",
    "c = [data[2], ]\n",
    "d = [data[3], ]\n",
    "e = [data[4], ]\n",
    "f = [data[5], ]\n",
    "g = [data[6], ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the following function to plot the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, labels, xlim, ylim):\n",
    "    \"\"\" Function to plot a list of (x,y) coordinates, with optional lables. \n",
    "        \n",
    "        Args:  \n",
    "              data     A list of (x,y) point to plot\n",
    "              labels   A list of text labels, in same order as data.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for point, label in zip(data, labels):\n",
    "        ax.scatter(point[0], point[1], c='black')\n",
    "        if labels: ax.annotate(label, point, textcoords=\"offset points\", xytext=(1,10))\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.grid(True)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(data, labels, xlim=(0,8), ylim=(0,6))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Write a custom function in pure Python to calculate the Euclidean straight-line distance between any two points with (x,y) coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point_a, point_b):\n",
    "    \"\"\" Function to calculate and return the Euclidean straight distance between any two points with (x,y) coordinates.\n",
    "        \n",
    "        Args:  \n",
    "               point_a   (x,y) of point a\n",
    "               point_b   (x,y) of point b\n",
    "    \"\"\"\n",
    "    return np.sqrt((point_a[0]-point_b[0])**2+(point_a[1]-point_b[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Write a custom function in pure Python to calculate a Euclidean distance matrix between a list of points; return a quasi-matrix as a pure Python \"list of lists.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(points):\n",
    "    \"\"\" Function to calculate the point to point distances between a list of points.\n",
    "        Return values in a quasi-\"matrix\" in the pure python form of a list-of-lists.\n",
    "        \n",
    "        Args:      \n",
    "              points      A list of coordinates. For example: [(1,3), (4,9), (5,7)]\n",
    "    \"\"\"\n",
    "    n = len(points)\n",
    "    arr = [[] for x in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            arr[i].append(distance(points[i],points[j]))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Write a custom function in pure Python to calculate the centroid of a cluster of coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(points):\n",
    "    \"\"\" Function to calculate and return the (x,y) coordinates for the centroid of a list of such (x,y) coordinates.\n",
    "        Args:  \n",
    "              points      A list of coordinates for the cluster. For example: [(1,3), (4,9), (5,7)]\n",
    "    \"\"\"\n",
    "    n = len(points)\n",
    "    x_avg = sum([x[0] for x in points])/n\n",
    "    y_avg = sum([x[1] for x in points])/n\n",
    "    return (x_avg,y_avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Write a custom function in pure Python to calculate the Euclidean distance between the closest points between two different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(cluster_a, cluster_b):\n",
    "    \"\"\" Function to calculate and return the Euclidean distance between the closest points between two different clusters.\n",
    "        \n",
    "        Args:  \n",
    "              points      A list of coordinates for the cluster. For example: [(1,3), (4,9), (5,7)]\n",
    "    \"\"\"\n",
    "    min_dist = math.inf\n",
    "    for point_a in cluster_a:\n",
    "        for point_b in cluster_b:\n",
    "            if distance(point_a, point_b)<min_dist:\n",
    "                min_dist=distance(point_a, point_b)\n",
    "                \n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Write a custom function in pure Python to calculate the distortion of a cluster. Define \"distortion\" as the sum of the squared distances of each point to the centroid of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion(points):\n",
    "    \"\"\" Function to calculate and return the distortion of a cluster. \n",
    "        Distortion is defined as the sum of the squared distances of each point to the centroid of the cluster.\n",
    "        \n",
    "        Args:  \n",
    "              points      A list of coordinates for the cluster. For example: [(1,3), (4,9), (5,7)]\n",
    "    \"\"\"\n",
    "    center = centroid(points)\n",
    "    return sum([distance(point,center)**2 for point in points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**:  Use the custom distance_matrix() function coded above to calculate the distance matrix between all observations. Try to print this out in a readable format (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0, 4.0, 6.0, 4.0, 3.605551275463989, 5.0],\n",
       " [1.0, 0.0, 3.0, 5.0, 4.123105625617661, 3.1622776601683795, 4.47213595499958],\n",
       " [4.0, 3.0, 0.0, 2.0, 5.656854249492381, 3.605551275463989, 4.123105625617661],\n",
       " [6.0, 5.0, 2.0, 0.0, 7.211102550927978, 5.0, 5.0],\n",
       " [4.0,\n",
       "  4.123105625617661,\n",
       "  5.656854249492381,\n",
       "  7.211102550927978,\n",
       "  0.0,\n",
       "  2.23606797749979,\n",
       "  3.0],\n",
       " [3.605551275463989,\n",
       "  3.1622776601683795,\n",
       "  3.605551275463989,\n",
       "  5.0,\n",
       "  2.23606797749979,\n",
       "  0.0,\n",
       "  1.4142135623730951],\n",
       " [5.0, 4.47213595499958, 4.123105625617661, 5.0, 3.0, 1.4142135623730951, 0.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calc and print distance matrix between all observations\n",
    "distance_matrix(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: We started with seven \"clusters\" with one observation in each cluster. Now use hierarchical agglomerative clustering to fuse the data together (going from the bottom up). Use the distance matrix above to help you perform this step. You may do it manually (by inspecting the matrix) or you may do it programmatically. Once you decide the proper order of fusing clusters, make variable assignments by updating the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the next 6 clusters by defining a variable that holds a list of all the points in that cluster\n",
    "___ =  ___ + ___\n",
    "___ =  ___ + ___\n",
    "___ =  ___ + ___\n",
    "___ =  ___ + ___\n",
    "___ =  ___ + ___\n",
    "___ =  ___ + ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Summarize the clustering process by printing out values for the items in the cluser, the separation distance between the old relevant clusters before fusing, the internal distortion within the new cluster after fusing, and the members of the new cluster after fusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the following values for each of the clusters with 2 or more observations in them\n",
    "print('CLUSTER'.center(10), 'DISTANCE'.center(10), 'DISTORTION'.center(20), 'MEMBERS')\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Plot the data again, but this time also plot a rectangle around each cluster, fusing clusters upward until everything is in just one cluster. TIP: You can use the plot_data() function from before, and the plot_cluster() function below, to help you with the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this function to add an illustrative box around a given cluster of points in a plot\n",
    "def plot_cluster(ax, cluster):\n",
    "    \"\"\" Function to add an illustrative box around a given cluster of points in a plot.\n",
    "    \n",
    "        Args:  \n",
    "              ax        An axis object from calling plt.subplots() in matplotlib\n",
    "              cluster   A list of coordinates for the cluster. For example: [(1,3), (4,9), (5,7)]\n",
    "    \"\"\"\n",
    "    padding = len(cluster)**2 * 0.05\n",
    "    min_x   = min([x for (x,y) in cluster]) - 0.5 - padding\n",
    "    max_x   = max([x for (x,y) in cluster]) - 0.5 + padding\n",
    "    min_y   = min([y for (x,y) in cluster]) - 0.5 - padding\n",
    "    max_y   = max([y for (x,y) in cluster]) - 0.5 + padding\n",
    "    width   = max_x - min_x + 1  \n",
    "    height  = max_y - min_y + 1.25  \n",
    "    ax.add_patch(patches.Rectangle((min_x,min_y), width, height, linewidth=2, edgecolor='r',facecolor='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data and plot a box around each hierarchical cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Using the data you have calculated above, now plot a dendogram below. You may do this manually usinge the `images/A3-Fig-2.png` file and editing it in an image editing program (save your edits to that file, and commit changes to your repository). Or try to use special functions in matplot lib to plot it directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/A3-Dendogram.png\" width=\"800\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Using your dendogram, report which observations would be grouped together if you had three clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1: ____________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 2: ____________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 3: ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Using your dendogram, report which observations would be grouped together if you had five clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1: ____________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 2: ____________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 3: ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 4: ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 5: ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PART 4**: NOW WE RETURN TO CREDIT DEFAULT PREDICTIONS AS IN ASSIGNMENT 2\n",
    "Repeat the analysis using a logit model. You can start with the code below, as it shows you how to setup, fit, and predict with a logit model in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now return to the problem of trying to predict the probability of defaulting on a credit card account at a Taiwanese bank. A credit card default happens when a customer fails to pay the minimum due on a credit card bill for more than 6 months. \n",
    "\n",
    "We will use a dataset from a Taiwanese bank with 30,000 observations (Source: *Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.*). Each observation represents an account at the bank at the end of October 2005.  We renamed the variable default_payment_next_month to customer_default. The target variable to predict is `customer_default` -- i.e., whether the customer will default in the following month (1 = Yes or 0 = No). The dataset also includes 23 other explanatory features. \n",
    "\n",
    "Variables are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| customer_default | Binary        | 1 = default in following month; 0 = no default \n",
    "| LIMIT_BAL        | Continuous    | Credit limit   \n",
    "| SEX              | Categorical   | 1 = male; 2 = female\n",
    "| EDUCATION        | Categorical   | 1 = graduate school; 2 = university; 3 = high school; 4 = others\n",
    "| MARRIAGE         | Categorical   | 0 = unknown; 1 = married; 2 = single; 3 = others\n",
    "| AGE              | Continuous    | Age in years  \n",
    "| PAY1             | Categorical   | Repayment status in September, 2005 \n",
    "| PAY2             | Categorical   | Repayment status in August, 2005 \n",
    "| PAY3             | Categorical   | Repayment status in July, 2005 \n",
    "| PAY4             | Categorical   | Repayment status in June, 2005 \n",
    "| PAY5             | Categorical   | Repayment status in May, 2005 \n",
    "| PAY6             | Categorical   | Repayment status in April, 2005 \n",
    "| BILL_AMT1        | Continuous    | Balance in September, 2005  \n",
    "| BILL_AMT2        | Continuous    | Balance in August, 2005  \n",
    "| BILL_AMT3        | Continuous    | Balance in July, 2005  \n",
    "| BILL_AMT4        | Continuous    | Balance in June, 2005 \n",
    "| BILL_AMT5        | Continuous    | Balance in May, 2005  \n",
    "| BILL_AMT6        | Continuous    | Balance in April, 2005  \n",
    "| PAY_AMT1         | Continuous    | Amount paid in September, 2005\n",
    "| PAY_AMT2         | Continuous    | Amount paid in August, 2005\n",
    "| PAY_AMT3         | Continuous    | Amount paid in July, 2005\n",
    "| PAY_AMT4         | Continuous    | Amount paid in June, 2005\n",
    "| PAY_AMT5         | Continuous    | Amount paid in May, 2005\n",
    "| PAY_AMT6         | Continuous    | Amount paid in April, 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurement scale for repayment status is:   \n",
    "\n",
    "    -2 = payment two months in advance   \n",
    "    -1 = payment one month in advance   \n",
    "    0 = pay duly   \n",
    "    1 = payment delay for one month   \n",
    "    2 = payment delay for two months   \n",
    "    3 = payment delay for three months   \n",
    "    4 = payment delay for four months   \n",
    "    5 = payment delay for five months   \n",
    "    6 = payment delay for six months   \n",
    "    7 = payment delay for seven months   \n",
    "    8 = payment delay for eight months   \n",
    "    9 = payment delay for nine months or more  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup AGAIN**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We purposefully have two different setup sections for this assignment. In the sections above you could not use sklearn, so we did not want to import those libraries. Now you CAN use sklearn.\n",
    "\n",
    "**From now on in this assignment, you CAN use sklearn !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import itertools\n",
    "import pandas_profiling\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.dummy           import DummyClassifier\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.ensemble        import GradientBoostingClassifier\n",
    "\n",
    "# Supporting functions from scikit-learn\n",
    "from sklearn.metrics         import confusion_matrix\n",
    "from sklearn.metrics         import roc_curve\n",
    "from sklearn.metrics         import roc_auc_score\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree            import export_graphviz\n",
    "from sklearn.decomposition   import PCA\n",
    "\n",
    "# ignore some warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for replication\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a few \"helper functions\" to automate repetitive tasks that we will perform below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=[0,1], normalize=False, title='Confusion Matrix', cmap=plt.cm.Reds):\n",
    "    \"\"\" \n",
    "    Function to plot a sklearn confusion matrix, showing number of cases per prediction condition \n",
    "    \n",
    "    Args:\n",
    "        cm         an sklearn confusion matrix\n",
    "        classes    levels of the class being predicted; default to binary outcome\n",
    "        normalize  apply normalization by setting `normalize=True`\n",
    "        title      title for the plot\n",
    "        cmap       color map\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round (cm[i, j],2), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, title='ROC Curve', note=''):\n",
    "    \"\"\"\n",
    "    Function to plot an ROC curve in a consistent way.\n",
    "    \n",
    "    Args:\n",
    "        fpr        False Positive Rate (list of multiple points)\n",
    "        tpr        True Positive Rate (list of multiple points)\n",
    "        title      Title above the plot\n",
    "        note       Note to display in the bottom-right of the plot\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title(title)\n",
    "    if note: plt.text(0.6, 0.2, note)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importance(tree_model, feature_names):\n",
    "    \"\"\"\n",
    "    Function to print a list of features from an sklearn tree model (ranked by importance of the feature)\n",
    "    \n",
    "    Args:\n",
    "        tree_model       A sklearn DecisionTreeClassifier()\n",
    "        feature_names    A list of features used by the DecisionTreeClassifier\n",
    "    \"\"\"\n",
    "    print('Feature'.center(12), '   ',  'Importance')\n",
    "    print('=' * 30)\n",
    "    for index in reversed(np.argsort(tree_model.feature_importances_)):\n",
    "        print(str(feature_names[index]).center(12) , '   ', '{0:.4f}'.format(tree_model.feature_importances_[index]).center(8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load and Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv('data/credit_data.csv')\n",
    "\n",
    "# One-hot-encode SEX and MARRIAGE  \n",
    "data = pd.get_dummies(data=data, columns=['SEX', 'MARRIAGE'])\n",
    "\n",
    "# Sort Columns alphabetically\n",
    "data = data.reindex(sorted(data.columns), axis=1)\n",
    "\n",
    "# Move target variable to first column (not necessary, but easier to see)\n",
    "data = data.set_index('customer_default').reset_index() \n",
    "\n",
    "# Select target\n",
    "y = np.array(data['customer_default'])\n",
    "\n",
    "# Select features \n",
    "features = list(set(list(data.columns)) - set(['customer_default']))\n",
    "X = data.loc[:, features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Discussion of Models and Results from Assignment 2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might we do to improve our prediction models from Assignment 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Note that the logit model in Assignment 2 did WORSE than the baseline model in Assignment 2 (at least when predicting class labels with a default probability threshold and when evaluated by the total number of errors made by the model). Using accuracy to compare models, however, as two drawbacks for the credit default problem. Describe those problems and potential solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ________ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: When do we need to standardize data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ________ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: When do we need to use a _Pipeline_? (And what is an sklearn Pipeline, anyway?) TIP: You will need to do some self-directed learning about pipelines in sklearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ________ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Was our training/testing method in Assignment 2 robust? What would make it more robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ________ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: For all of the problems that follow, divide the data into three parts:  **training** (60%), **validation** (20%) and **test** (20%). In the python code, refer to those subsets as: \n",
    "\n",
    "| Subset      |  Pct.  |  X code var     | Target code var |\n",
    "|-------------|--------|-----------------|-----------------|\n",
    "| training    |  60%   |  X_train_train  | y_train_train\n",
    "| validation  |  20%   | X_train_val     | y_train_val\n",
    "| testing     |  20%   | X_test          | y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "\n",
    "X_train, X_test, y_train, y_test                        = _____________________\n",
    "\n",
    "X_train_train, X_train_val, y_train_train, y_train_val  = _____________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PART 5**: Logit Model with L1 Regularization (\"Lasso\")\n",
    "Predict credit default using a logit model. We give you a start with the code below, as it shows you how to setup, fit, and predict with a pipeline in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline   \n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))         # tell it to standardize features\n",
    "estimators.append(('logit_model_l1', LogisticRegression()))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators) \n",
    "pipeline.set_params(logit_model_l1__penalty='l1')            # tell it to regularize with L1 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune for the optimal amount of regularization (the tuning parameter `c` in sklearn). We will again give you this code to show you the general pattern for how to setup hyper-parameter tuning in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune C  \n",
    "results = []\n",
    "for c in np.logspace(-4, 5, 10):\n",
    "    pipeline.set_params(logit_model_l1__C=c) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)       # use validation set during hyper-parameter tuning\n",
    "    auc_lml1 = roc_auc_score(y_train_val, y_train_pred[:,1])   \n",
    "    results.append( (auc_lml1, c)  )\n",
    "logit_model_l1 = pipeline.named_steps['logit_model_l1']      # capture model so we can use it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Make a table of the results from hyper-parameter tuning so you can inspect them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Pick what you believe to be the \"best\" value for c. Justify your selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best C  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ________ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Test the final model, using the selected best value for C. Report the final testing score and plot the ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Compare coefficients between a standard logit model and a regularized logit model, to see how Lasso drops predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients between a standard logit model and a regularized logit model, to see how Lasso drops predictors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Comment on which predictors dropped, and why. HINT: Maybe plot a fast-and-rough correlation matrix to test a hunch if you have a hunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "\n",
    "\n",
    "    \n",
    "# Make sorted list by absolute value\n",
    "\n",
    "\n",
    "    \n",
    "# Print correlations in rank order descending\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: ________ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PART 6**: PCA\n",
    "As an interesting learning exercise, visualize ALL your data in 2 dimensions using PCA. PCA is only able to capture linear variations in the data. For non-linear projections, one could also use T-SNE. However, T-SNE has more tunable parameters compared to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Use the sklearn StandardScaler() to standardize all of your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize ALL features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Use the sklearn PCA() function to project ALL features into just two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project features into two dimensions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Explore the relationship between the AUC from a standard Logit Model (withOUT using Lasso) and the number of PCA Component Features being used as predictors in the model. In other words, build a `for` loop that tries many numbers of components from PCA in a basic Logit Model.  \n",
    "\n",
    "Hint: you will need to use code like the following in order to build up a pipeline comprised of the StandardScaler(), PCA(), and LogisticRegression() steps.\n",
    "\n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append(('pca', PCA()))\n",
    "    estimators.append(('model', LogisticRegression()))\n",
    "    pipeline = Pipeline(estimators)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore and print out results for the relationship between AUC and the number of PCA Component Features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Plot your incremental results (increasing levels of components) from the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your incremental results (increasing levels of components) from the steps above.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PART 7**:  K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to help you get going, we will set up a basic pipeline for hyperparameter tuning K for you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('knn_model', KNeighborsClassifier()))\n",
    "pipeline = Pipeline(estimators) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Tune the KNN model to find the optimal K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune K\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# View results \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: What was the best K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best K\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Test your final model and report the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PART 8**: Decision Trees\n",
    "\n",
    "Unlike a logit model or a KNN classifier, decision trees are not sensitive to the scaling of categorical features and numeric features. They can find a cut point in arbitrary numeric or categorical features. The implementation of decision trees in sklearn, however, does require categorical features to be one-hot-encoded, even though that is technically not required by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Fit just one tree using the `DecisionTreeClassifier()` function from sklearn. There is no need to use a pipeline for this step, as it is just one tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a one-tree Model      \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Test the model and report the AUC. HINT: Make sure you are using the test sets every time you do a final test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Plot the resulting ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Use the `print_feature_importance()` function given to you above to print the relative feature importances of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PART 8**: Random Forests\n",
    "\n",
    "Just one tree may be an arbitrary and unreliable model. So next, use the `RandomForestClassifier()` model from sklearn to run a random forest of trees and average across them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Build a pipeline for a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Tune for N, the number of trees and print out the results so you can find the best N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune N   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# View results \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Test the model and report the AUC. HINT: Make sure you are using the test sets every time you do a final test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Plot the resulting ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**: Use the `print_feature_importance()` function given to you above to print the relative feature importances of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importance\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
